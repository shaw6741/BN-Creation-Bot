{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a88a309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.factors.discrete.CPD import TabularCPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "227763c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = \"YOUR_API_KEY\"\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b976da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bold Text fuction when printing\n",
    "def bold_text(text):\n",
    "    return \"\\033[1m\" + text + \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c825cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_nodes = [node for node in missing_node_list if node not in unique_events]\n",
    "\n",
    "# for edge in missing_nodes:\n",
    "#     node_a = edge[0]\n",
    "#     node_b = edge[1]\n",
    "#     value_a = conversation_data.get('triggers', {}).get(node_a) or conversation_data.get('mitigators', {}).get(node_a)\n",
    "#     value_b = conversation_data.get('events', {}).get(node_b) or conversation_data.get('consequences', {}).get(node_b)\n",
    "#     missing_nodes_name.append([value_a, value_b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64b3b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember do not mix the main questions and follow-up questions!\n",
    "missing_nodes_name = 'Hedging' #change to Hedging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9131dd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "\u001b[1mChatGPT:\u001b[0m Q1. Which state has a lower probability: \"Hedging\" or Not Hedging?\n",
      "not hedging\n",
      "\u001b[1mChatGPT:\u001b[0m Q2. If the relative weight of not Hedging is 1, what is the relative weight of Hedging?\n",
      "3.5\n",
      "\u001b[1mChatGPT:\u001b[0m Based on the information provided:\n",
      "\n",
      "Hedging = 3.5\n",
      "Not Hedging = 1\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "template = f\"\"\"\n",
    "I want you to act as a person collecting information on the probability and relative weight.\n",
    "Ask questions one at a time, sequentially! Wait for my answer before moving on to the next question.\n",
    "\n",
    "Below is the instruction on asking questions.\n",
    "Q1. Which state has a lower probability: \"{missing_nodes_name}\" or Not {missing_nodes_name}?\n",
    "Q2. The follow-up question you should ask is;\n",
    "If the answer is {missing_nodes_name}, ask \n",
    "\"If relative weight of {missing_nodes_name} is 1, what is the relative weight of Not {missing_nodes_name}?\".\n",
    "If the answer is not {missing_nodes_name}, ask \n",
    "\"If relative weight of not {missing_nodes_name} is 1, what is the relative weight of {missing_nodes_name}?\".\n",
    "\n",
    "After you've collected all the information on relative weight value, summarize it as follows; \n",
    "\n",
    "{missing_nodes_name} = Relative weight numeric value\n",
    "Not {missing_nodes_name} = Relative weight numeric value\n",
    "\n",
    "An example summary is as follows;\n",
    "{missing_nodes_name} = 1\n",
    "Not {missing_nodes_name} = 3\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chat_log = []\n",
    "\n",
    "while True:\n",
    "    user_message = input()\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        chat_log.append({\"role\": \"user\", \"content\": user_message})\n",
    "        # Append the template as a system message in the chat log\n",
    "        chat_log.append({\"role\": \"system\", \"content\": template})\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model= \"gpt-3.5-turbo-16k-0613\",\n",
    "            messages=chat_log,\n",
    "            temperature=0.8\n",
    "        )\n",
    "\n",
    "        assistant_response = response['choices'][0]['message']['content']\n",
    "        print(bold_text(\"ChatGPT:\"), assistant_response.strip(\"\\n\").strip())\n",
    "        chat_log.append({\"role\": \"assistant\", \"content\" : assistant_response.strip(\"\\n\").strip()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2bc6d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "\u001b[1mChatGPT:\u001b[0m Q1. Which state has a lower probability: \"Hedging\" or Not Hedging?\n",
      "not hedging\n",
      "\u001b[1mChatGPT:\u001b[0m Q2. If the relative weight of \"Not Hedging\" is 1, what is the relative weight of \"Hedging\"?\n",
      "2.5\n",
      "\u001b[1mChatGPT:\u001b[0m Based on the information provided, the summary is as follows:\n",
      "\n",
      "Hedging = 2.5\n",
      "Not Hedging = 1\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "template = f\"\"\"\n",
    "I want you to act as a person collecting information on the probability and relative weight.\n",
    "Ask questions one at a time, sequentially!\n",
    "Wait for my answer before moving on to the next question.\n",
    "\n",
    "Below is the instruction on asking questions.\n",
    "Q1. Which state has a lower probability: \"{missing_nodes_name}\" or Not {missing_nodes_name}?\n",
    "Q2. The follow-up question you should ask is;\n",
    "Based on the lower probability state, you should ask \n",
    "\"If relative weight of (previous answer) is 1, what is the relative weight of (opposite of previous answer)?\". \n",
    "\n",
    "\n",
    "After you've collected all the information on relative weight value, summarize as follows:\n",
    "{missing_nodes_name} = Relative weight numeric value\n",
    "Not {missing_nodes_name} = Relative weight numeric value\n",
    "\n",
    "An example summary is as follows;\n",
    "{missing_nodes_name} = 1\n",
    "Not {missing_nodes_name} = 3\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chat_log = []\n",
    "\n",
    "while True:\n",
    "    user_message = input()\n",
    "    if user_message.lower() == \"quit\":\n",
    "        break\n",
    "    else:\n",
    "        chat_log.append({\"role\": \"user\", \"content\": user_message})\n",
    "        # Append the template as a system message in the chat log\n",
    "        chat_log.append({\"role\": \"system\", \"content\": template})\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model= \"gpt-3.5-turbo-16k-0613\",\n",
    "            messages=chat_log,\n",
    "            temperature=0.5 #0.3\n",
    "        )\n",
    "\n",
    "        assistant_response = response['choices'][0]['message']['content']\n",
    "        print(bold_text(\"ChatGPT:\"), assistant_response.strip(\"\\n\").strip())\n",
    "        chat_log.append({\"role\": \"assistant\", \"content\" : assistant_response.strip(\"\\n\").strip()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba179156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_eigenvector(assistant_response):\n",
    "    numeric_values = [float(value) for value in re.findall(r\"=\\s(\\d+\\.\\d+|\\d+)\", assistant_response)]\n",
    "\n",
    "    nested_array = [\n",
    "        [1, numeric_values[0]],\n",
    "        [1/numeric_values[0], 1],\n",
    "    ]\n",
    "\n",
    "    matrix = np.array(nested_array)\n",
    "    \n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "\n",
    "    max_eigenvalue_index = np.argmax(eigenvalues)\n",
    "    max_eigenvalue = eigenvalues[max_eigenvalue_index]\n",
    "\n",
    "    max_eigenvector = np.real(eigenvectors[:, max_eigenvalue_index])\n",
    "\n",
    "    # Normalize the eigenvector\n",
    "    max_eigenvector /= np.sum(max_eigenvector)\n",
    "    max_eigenvector_2d = np.array([[value] for value in max_eigenvector])\n",
    "    \n",
    "    # Calculate CR\n",
    "    n = len(matrix)  # Number of criteria (dimensions) in the matrix\n",
    "    CI = (max_eigenvalue - n) / (n - 1)\n",
    "    RI = 0.58\n",
    "    CR = CI / RI\n",
    "\n",
    "    return matrix, max_eigenvector_2d, CR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc946f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, max_eigenvector_2d, CR = calculate_max_eigenvector(assistant_response)\n",
    "\n",
    "H_states = ['Opting', 'Not Opting']\n",
    "H_cpd = TabularCPD(variable='Hedge', variable_card=2, values=max_eigenvector_2d, state_names={'Hedge': H_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fa700397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "| Hedge(Opting)     | 0.714286 |\n",
      "+-------------------+----------+\n",
      "| Hedge(Not Opting) | 0.285714 |\n",
      "+-------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "print(H_cpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = [[0.1, 0.5, 0.3], [0.2, 0.7, 0.5], [0.4, 0.8, 0.6], [0.7, 1.0, 0.9]]\n",
    "\n",
    "# Save the array to a JSON file\n",
    "with open('my_array.json', 'w') as file:\n",
    "    json.dump(my_array, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4cc4860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix, max_eigenvector_2d, CR = calculate_max_eigenvector(assistant_response)\n",
    "\n",
    "directory_path = \"/Users/irislee/Uchicago Class Materials/4. 3Q/Capstone/Langchain/Chat\"\n",
    "file_name = \"prior_prob.json\"\n",
    "file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "# Convert the matrices ndarray to Python lists\n",
    "matrix_list = matrix.tolist()\n",
    "\n",
    "# Save the Python lists to a JSON file\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(matrix_list, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6684589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.5], [0.4, 1.0]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1fd2f9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71428571],\n",
       "       [0.28571429]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix_list\n",
    "max_eigenvector_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if the matrix is acceptable based on CR\n",
    "# if CR <= 0.1:\n",
    "#     print(\"The matrix is acceptable.\")\n",
    "# else:\n",
    "#     print(\"The matrix is not acceptable.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
